{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nombre: \n",
    "Herrera Gandarela Gabriel Alejandro\n",
    "#### Materia: \n",
    "Análisis y Procesamiento Inteligente de Textos\n",
    "#### Ejercicio:\n",
    "Creación de un sistema de clasificación de lengua basado en caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza del corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpieza(corpus):\n",
    "    nltk.download('punkt')\n",
    "\n",
    "    sents =  [word_tokenize(s) for s in sent_tokenize(open(corpus,'r', encoding='utf8').read())]\n",
    "    #Separamos por palabras#\n",
    "    #limpieza_corpus = [w for w in sents if w.isalpha()] \n",
    "    stopwords_list = stopwords.words('spanish')\n",
    "    #print(limpieza_corpus)\n",
    "    #Expresión regular para limpieza de signos\n",
    "    exp_reg_signal = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    clean_signal = [exp_reg_signal.sub('', str(w)) for w in sents]\n",
    "    #print(clean_signal)\n",
    "    ##Limpieza de espacios en blanco al final e inicio de la cadena\n",
    "    exp_reg_space = re.compile('^\\s+|\\s+$')\n",
    "    clean_space = [exp_reg_space.sub('', w) for w in clean_signal]\n",
    "    #Limpieza de espacios duplicados en las cadenas\n",
    "    exp_reg_spaces = re.compile(r'\\s{2,}?')\n",
    "    clean_spaces = [exp_reg_spaces.sub('', w) for w in clean_space]\n",
    "    clean_corpus = clean_spaces\n",
    "    return clean_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nopcional\\nletras = []\\nfor i in corpus:\\n    pal = i.split(\\' \\')\\n    for j in pal:\\n        for k in range(0, len(j),1):\\n            #bigram.append(j[k])\\n            if(len(j[k:k+1])==0):\\n                #print(j[k:k+2]+\"_\")\\n                letras.append(j[k:k+1]+\"_\")\\n            else:\\n                letras.append(j[k:k+1])\\n'"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "opcional\n",
    "letras = []\n",
    "for i in corpus:\n",
    "    pal = i.split(' ')\n",
    "    for j in pal:\n",
    "        for k in range(0, len(j),1):\n",
    "            #bigram.append(j[k])\n",
    "            if(len(j[k:k+1])==0):\n",
    "                #print(j[k:k+2]+\"_\")\n",
    "                letras.append(j[k:k+1]+\"_\")\n",
    "            else:\n",
    "                letras.append(j[k:k+1])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "opcional\n",
    "bigram = []\n",
    "for i in corpus:\n",
    "    pal = i.split(' ')\n",
    "    for j in pal:\n",
    "        for k in range(0, len(j),2):\n",
    "            #bigram.append(j[k])\n",
    "            if(len(j[k:k+2])==1):\n",
    "                #print(j[k:k+2]+\"_\")\n",
    "                bigram.append(j[k:k+2]+\"_\")\n",
    "            else:\n",
    "                bigram.append(j[k:k+2])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función que convierte a ngramas el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_ngram(ngram, corpus, n):\n",
    "    for i in corpus:\n",
    "        pal = re.split('[a-z][A-Z]', i)\n",
    "        for j in pal:\n",
    "            for k in range(0, len(j),n):\n",
    "                ngram.append(j[k:k+n])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función que determina qué número es mayor\n",
    "La aplicaremos para determinar cual es la distancia mayor de acuerdo con la comparación entre cada corpus de entrenamiento del idioma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_mayor(num1, num2, num3):\n",
    "    if(num1>num2):\n",
    "        if(num1>num3):\n",
    "            print(\"EL idioma introducido es español\")\n",
    "    elif(num3>num2):\n",
    "        print(\"EL idioma introducido es aleman\")\n",
    "    else:\n",
    "        if(num2>num3):\n",
    "            print(\"EL idioma introducido es inglés\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valida_idioma(corpus_prueba):\n",
    "    n_gram = int(input(\"Ngrama que quieres calcular\")) \n",
    "    grama_entrena_es = [] #grama de entrenamiento para español \n",
    "    grama_entrena_en = [] ##grama de entrenamiento para ingles\n",
    "    grama_entrena_ale = [] ##grama de entrenamiento para aleman\n",
    "    grama_prueba = [] #Grama de prueba\n",
    "    #para cada corpus, hacemos la limpieza y lo almacenamos en una lista\n",
    "    corpus_ngram(grama_entrena_es, limpieza('corpus/la_biblioteca_de_babel_esp.txt'),n_gram)\n",
    "    corpus_ngram(grama_entrena_en, limpieza('corpus/la_biblioteca_de_babel_en.txt'),n_gram)\n",
    "    corpus_ngram(grama_entrena_ale, limpieza('corpus/la_biblioteca_de_babel_ale.txt'),n_gram)\n",
    "    #creamos un ngrama para el corpus de prueba\n",
    "    corpus_ngram(grama_prueba, corpus_prueba,n_gram)\n",
    "    #calculamos la distancia entre cada idioma de acuerdo con el algoritmo de jacard\n",
    "    #recibe dos listas, una será el grama de entremiento y otro de prueba\n",
    "    #Calculamos las distancias para cada idioma\n",
    "    num_esp= jaccard(grama_entrena_es,grama_prueba)*1000\n",
    "    num_en= jaccard(grama_entrena_en,grama_prueba)*1000\n",
    "    num_ale= jaccard(grama_entrena_ale,grama_prueba)*1000\n",
    "    #ejecutamos la función que calcula la distancia mayor\n",
    "    #entre mayor sea la distancia, significa que ese será el idioma seleccionado\n",
    "    num_mayor(num_esp, num_en, num_ale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_esp = ['el perro come un hueso', 'un muchacho jugaba', 'el muchacho saltaba la cuerda',\n",
    "          'un gato come croquetas']\n",
    "corpus_en = ['The dog eats a bone', 'a boy played', 'the boy was jumping rope', 'a cat eats croquettes']\n",
    "corpus_ale = ['Der Hund frisst einen Knochen', 'ein Junge spielte', 'der Junge war Seilspringen', 'eine katze isst kroketten']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngrama que quieres calcular3\n",
      "EL idioma introducido es aleman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"corpus_prueba = []\n",
    "for num in range(0,3):\n",
    "    oracion = input(\"Escribe la oracion \"+str(num+1))\n",
    "    corpus_prueba.append(oracion)  \"\"\"  \n",
    "#prueba de corpus_\n",
    "valida_idioma(corpus_ale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "frec_letras = Counter(letras)\n",
    "frec_bigram = Counter(bigrama)\n",
    "frec_trigram = Counter(trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función que calcula la distancia entre dos listas \n",
    "https://www.statology.org/jaccard-similarity-python/\n",
    "\n",
    "\n",
    "Apuntes del Profesor Víctor Mijangos de la materia Procesamiento Inteligente de Textos\n",
    "https://github.com/VMijangos/Curso-Procesamiento-de-Lenguaje-Natural/tree/master/Notebooks\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
